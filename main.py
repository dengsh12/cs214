# main.py
from multiprocessing import Process
import time
import threading
from threading import Thread
import statistics
import argparse
from confluent_kafka.admin import AdminClient, NewTopic
from producer import produce_messages
from consumer import consume_messages
import utility

def delete_topic(admin_client, topic_name):
    """åˆ é™¤ Kafka ä¸»é¢˜"""
    print(f"å°è¯•åˆ é™¤ Topic: {topic_name}")
    topic_metadata = admin_client.list_topics(timeout=10)
    if topic_name not in topic_metadata.topics:
        print(f"Topic {topic_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
        return
    fs = admin_client.delete_topics([topic_name], operation_timeout=30)

    for topic, f in fs.items():
        try:
            print(f"ç­‰å¾… Topic {topic} åˆ é™¤")
            f.result()  # é˜»å¡ç›´åˆ°åˆ é™¤å®Œæˆ
            print(f"âœ… Topic {topic} åˆ é™¤æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Topic {topic} åˆ é™¤å¤±è´¥: {e}")
    print()

    # ç­‰å¾…ä¸»é¢˜å®Œå…¨åˆ é™¤
    while True:
        topic_metadata = admin_client.list_topics(timeout=10)
        if topic_name not in topic_metadata.topics:
            break
        print(f"âš ï¸ Topic {topic_name} ä»åœ¨åˆ é™¤ä¸­ï¼Œç­‰å¾…...")
    time.sleep(5)  # ç­‰å¾…5ç§’

def create_topic(admin_client, topic_name, num_partitions=3, replication_factor=1):
    """
    æ£€æŸ¥å¹¶åˆ›å»º Kafka ä¸»é¢˜ï¼Œå¦‚æœå·²ç»å­˜åœ¨åˆ™è·³è¿‡ã€‚
    :param admin_client: AdminClient å®ä¾‹
    :param topic_name: è¦åˆ›å»ºçš„ Topic åç§°
    :param num_partitions: åˆ†åŒºæ•°
    :param replication_factor: å‰¯æœ¬å› å­
    """
    # 1. å…ˆæ£€æŸ¥ Topic æ˜¯å¦å·²å­˜åœ¨
    topic_metadata = admin_client.list_topics(timeout=10)
    if topic_name in topic_metadata.topics:
        print(f"âš ï¸ Topic {topic_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
        return

    # 2. å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»º Topic
    print(f"ğŸš€ åˆ›å»º Topic: {topic_name}")
    topic_list = [
        NewTopic(topic_name, 
                 num_partitions=num_partitions, 
                 replication_factor=replication_factor)
    ]

    fs = admin_client.create_topics(topic_list, operation_timeout=30)
    for t, f in fs.items():
        try:
            f.result()  # é˜»å¡ç›´åˆ°åˆ›å»ºå®Œæˆæˆ–æŠ¥é”™
            print(f"âœ… Topic {t} åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Topic {t} åˆ›å»ºå¤±è´¥: {e}")
    print()

if __name__ == '__main__':
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Kafka ååé‡æµ‹è¯•")
    parser.add_argument("--batch_size", type=int, default=16384, help="ç”Ÿäº§è€…æ‰¹å¤„ç†å¤§å° (bytes)")
    parser.add_argument("--linger_ms", type=int, default=5, help="ç”Ÿäº§è€… linger.ms (ms)")
    parser.add_argument("--broker_address", type=str, default="localhost:9092", help="Kafka broker åœ°å€")
    parser.add_argument("--num_message", type=int, default=2000, help="å‘é€çš„æ¶ˆæ¯æ€»æ•°")

    args = parser.parse_args()

    # 2. Kafka åŸºæœ¬é…ç½®
    KAFKA_BROKER = args.broker_address
    TOPIC = "test-throughput"
    NUM_MESSAGES = args.num_message
    # LOG_INTERVAL = NUM_MESSAGES // 100 if NUM_MESSAGES >= 100 else 1
    LOG_INTERVAL = NUM_MESSAGES // 10 if NUM_MESSAGES >= 100 else 1

    # 3. AdminClientï¼Œç”¨äºåˆ›å»º/åˆ é™¤ Topic
    admin_client = AdminClient({"bootstrap.servers": KAFKA_BROKER})

    # æ‰“å°æ˜¯æœ¬åœ°è¿˜æ˜¯äº‘ç¯å¢ƒ
    if not KAFKA_BROKER.startswith("localhost"):
        print("ğŸš€ äº‘ç¯å¢ƒ Kafka Broker åœ°å€:", KAFKA_BROKER)
    else:
        print("ğŸš€ æœ¬åœ°ç¯å¢ƒ Kafka Broker åœ°å€:", KAFKA_BROKER)

    # 4. åˆ é™¤å¹¶é‡æ–°åˆ›å»ºæµ‹è¯• Topic
    delete_topic(admin_client, TOPIC)
    create_topic(admin_client, TOPIC)

    # 5. é…ç½®ç”Ÿäº§è€… & æ¶ˆè´¹è€…
    producer_conf = {
        'compression.type': 'lz4', # æŒ‡å®šå‹ç¼©ç®—æ³•
        'bootstrap.servers': KAFKA_BROKER,
        'acks': 'all',
        'batch.size': args.batch_size,
        'linger.ms': args.linger_ms
    }
    consumer_conf = {
        'bootstrap.servers': KAFKA_BROKER,
        'group.id': "latency-test-group",
        'auto.offset.reset': 'earliest'
    }

    # 6. å‡†å¤‡æ”¶é›†å»¶è¿Ÿæ•°æ®
    latencies = []
    start_time = time.time()

    # 7. å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹å’Œæ¶ˆè´¹è€…è¿›ç¨‹
    # producer_t = Thread(target=produce_messages, args=(producer_conf, TOPIC, NUM_MESSAGES, LOG_INTERVAL))
    # consumer_t = Thread(target=consume_messages, args=(consumer_conf, TOPIC, NUM_MESSAGES, LOG_INTERVAL, latencies))
    producer_t = Process(target=produce_messages, args=(producer_conf, TOPIC, NUM_MESSAGES, LOG_INTERVAL))
    # consumer_t = Process(target=consume_messages, args=(consumer_conf, TOPIC, NUM_MESSAGES, LOG_INTERVAL, latencies))
    
    producer_t.start()
    # time.sleep(3)  # è®©ç”Ÿäº§è€…å…ˆå‘ä¸€ä¼šï¼Œå†å¯åŠ¨æ¶ˆè´¹è€…
    # consumer_t.start()
    # è¿™é‡Œçš„latencieså¤„ç†é€»è¾‘è¿˜æœ‰é—®é¢˜
    latencies = consume_messages(consumer_conf, TOPIC, NUM_MESSAGES, LOG_INTERVAL, latencies)

    producer_t.join()
    # consumer_t.join()

    # 8. è®¡ç®—ååé‡å’Œå»¶è¿Ÿç»Ÿè®¡
    total_time = time.time() - start_time
    throughput = NUM_MESSAGES / total_time

    if latencies:
        import statistics
        avg_latency = statistics.mean(latencies)
        # è¿‘ä¼¼99åˆ†ä½æ•°
        p99_latency = statistics.quantiles(latencies, n=100)[98]  
        max_latency = max(latencies)

        print("\nğŸ“Š æµ‹è¯•å®Œæˆï¼š")
        print(f"âœ… å‘é€ {NUM_MESSAGES} æ¡æ¶ˆæ¯")
        print(f"ğŸš€ ååé‡: {throughput:.2f} æ¡/ç§’")
        print(f"â³ å¹³å‡å»¶è¿Ÿ: {avg_latency:.6f} ç§’")
        print(f"ğŸ”´ 99% å»¶è¿Ÿ: {p99_latency:.6f} ç§’")
        print(f"âš¡ æœ€å¤§å»¶è¿Ÿ: {max_latency:.6f} ç§’")
    else:
        print("âš ï¸ æ²¡æœ‰æ¥æ”¶åˆ°ä»»ä½•æ¶ˆæ¯ï¼Œæ— æ³•è®¡ç®—å»¶è¿Ÿç»Ÿè®¡ã€‚")
