import time
from multiprocessing import Process, Manager
import argparse
import requests
import os
import json

# Kafkaç›¸å…³
from confluent_kafka.admin import AdminClient, NewTopic
from producer import produce_messages
from consumer import consume_messages

# RocketMQç›¸å…³
from producer_rocketmq import produce_messages_rocketmq
from consumer_rocketmq import consume_messages_rocketmq
from rocketmq_admin import create_topic_rocketmq, delete_topic_rocketmq

import utility

def delete_topic_kafka(admin_client, topic_name):
    """åˆ é™¤ Kafka ä¸»é¢˜"""
    print(f"å°è¯•åˆ é™¤ Topic: {topic_name}")
    topic_metadata = admin_client.list_topics(timeout=10)
    if topic_name not in topic_metadata.topics:
        print(f"Topic {topic_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
        return
    fs = admin_client.delete_topics([topic_name], operation_timeout=30)
    for topic, f in fs.items():
        try:
            print(f"ç­‰å¾… Topic {topic} åˆ é™¤")
            f.result()
            print(f"âœ… Topic {topic} åˆ é™¤æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Topic {topic} åˆ é™¤å¤±è´¥: {e}")
    print()
    while True:
        topic_metadata = admin_client.list_topics(timeout=10)
        if topic_name not in topic_metadata.topics:
            break
        print(f"âš ï¸ Topic {topic_name} ä»åœ¨åˆ é™¤ä¸­ï¼Œç­‰å¾…...")
    time.sleep(5)

def create_topic_kafka(admin_client, topic_name, num_partitions=3, replication_factor=1):
    """åˆ›å»º Kafka ä¸»é¢˜"""
    topic_metadata = admin_client.list_topics(timeout=10)
    if topic_name in topic_metadata.topics:
        print(f"âš ï¸ Topic {topic_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
        return

    print(f"ğŸš€ åˆ›å»º Topic: {topic_name}")
    topic_list = [NewTopic(topic_name, num_partitions=num_partitions, replication_factor=replication_factor)]
    fs = admin_client.create_topics(topic_list, operation_timeout=30)
    for t, f in fs.items():
        try:
            f.result()
            print(f"âœ… Topic {t} åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Topic {t} åˆ›å»ºå¤±è´¥: {e}")
    print()

def start_remote_monitoring(remote_ips):
    """å¯¹æ¯ä¸ªæœåŠ¡å™¨è°ƒç”¨ /start_monitor æ¥å£ï¼Œç”¨äºé‡‡é›†èµ„æºä¿¡æ¯"""
    baseline_data = {}
    for ip in remote_ips:
        url = f"http://{ip}:5000/start_monitor"
        try:
            resp = requests.get(url, timeout=15)
            data = resp.json()
            baseline_data[ip] = data
            print(f"è¿œç¨‹ç›‘æ§å¯åŠ¨[{ip}]: {data}")
        except Exception as e:
            print(f"å¯åŠ¨è¿œç¨‹ç›‘æ§[{ip}]å¤±è´¥: {e}")
    return baseline_data

def stop_remote_monitoring(remote_ips):
    """å¯¹æ¯ä¸ªæœåŠ¡å™¨è°ƒç”¨ /stop_monitor æ¥å£ï¼Œè¿”å›å„æœåŠ¡å™¨çš„ç›‘æ§æ•°æ®"""
    results = []
    for ip in remote_ips:
        url = f"http://{ip}:5000/stop_monitor"
        try:
            resp = requests.get(url, timeout=15)
            data = resp.json()
            data["ip"] = ip
            results.append(data)
            print(f"è¿œç¨‹ç›‘æ§åœæ­¢[{ip}]: {data}")
        except Exception as e:
            print(f"åœæ­¢è¿œç¨‹ç›‘æ§[{ip}]å¤±è´¥: {e}")
    return results

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="MQ ååé‡ä¸å»¶è¿Ÿæµ‹è¯•")
    parser.add_argument("--mq_type", type=str, default="kafka", help="æ¶ˆæ¯é˜Ÿåˆ—ç±»å‹, kafka æˆ– rocketmq")
    parser.add_argument("--broker_address", type=str, default="localhost:9092", help="Kafka: host:port; RocketMQ: 'ip1:9876;ip2:9876'")
    parser.add_argument("--topic", type=str, default="test-throughput", help="æµ‹è¯• Topic åç§°")
    parser.add_argument("--num_producers", type=int, default=50, help="ç”Ÿäº§è€…æ•°é‡")
    parser.add_argument("--num_consumers", type=int, default=50, help="æ¶ˆè´¹è€…æ•°é‡")
    parser.add_argument("--messages_per_producer", type=int, default=1000, help="æ¯ä¸ªç”Ÿäº§è€…å‘é€çš„æ¶ˆæ¯æ•°é‡")
    parser.add_argument("--log_interval", type=int, default=100, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--remote_ips", type=str, default="", help="æœåŠ¡å™¨IPåˆ—è¡¨ï¼Œç”¨äºè¿œç¨‹èµ„æºç›‘æ§ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--message_size", type=int, default=100, help="æ¶ˆæ¯å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    args = parser.parse_args()

    remote_ips = [ip.strip() for ip in args.remote_ips.split(",") if ip.strip()]
    if not remote_ips:
        print("è¯·æŒ‡å®šæœåŠ¡å™¨IPåˆ—è¡¨ï¼ˆ--remote_ipsï¼‰ï¼Œç”¨äºé‡‡é›†èµ„æºæŒ‡æ ‡")
        exit(1)

    # æ€»æ¶ˆæ¯é‡
    total_messages = args.num_producers * args.messages_per_producer

    # Kafka æƒ…å†µä¸‹ï¼Œå¸¸è®© "åˆ†åŒºæ•° = æ¶ˆè´¹è€…æ•°"ï¼›RocketMQ åˆ™å¯è‡ªç”±é…ç½®é˜Ÿåˆ—æ•°
    messages_per_consumer = total_messages // args.num_consumers

    if args.mq_type.lower() == "kafka":
        # -- Kafka æµ‹è¯• --
        admin_client = AdminClient({"bootstrap.servers": args.broker_address})
        delete_topic_kafka(admin_client, args.topic)
        create_topic_kafka(admin_client, args.topic, num_partitions=args.num_consumers, replication_factor=1)

        baseline_metrics = start_remote_monitoring(remote_ips)

        from multiprocessing import Manager
        manager = Manager()
        producer_metrics = manager.list()
        consumer_metrics = manager.list()
        processes = []

        # å¯åŠ¨æ¶ˆè´¹è€…è¿›ç¨‹
        for i in range(args.num_consumers):
            p = Process(target=consume_messages, args=(
                {
                    'bootstrap.servers': args.broker_address,
                    'group.id': "latency-test-group",
                    'auto.offset.reset': 'earliest'
                },
                args.topic, messages_per_consumer, args.log_interval, consumer_metrics, i
            ))
            p.start()
            processes.append(p)

        # å¯åŠ¨ç”Ÿäº§è€…è¿›ç¨‹
        for i in range(args.num_producers):
            p = Process(target=produce_messages, args=(
                {
                    'bootstrap.servers': args.broker_address,
                    'acks': '1',
                    'batch.size': 16384,
                    'linger.ms': 5,
                    'compression.type': 'none'
                },
                args.topic, args.messages_per_producer, args.log_interval, producer_metrics, i, args.message_size
            ))
            p.start()
            processes.append(p)

        for p in processes:
            p.join()

        remote_results = stop_remote_monitoring(remote_ips)

    elif args.mq_type.lower() == "rocketmq":
        # -- RocketMQ æµ‹è¯• --
        namesrv_addr = args.broker_address
        delete_topic_rocketmq(args.topic, namesrv_addr)
        # åˆ†é…é˜Ÿåˆ—æ•°ä¸º num_consumers æˆ–è‡ªå·±è®¾ä¸€ä¸ªå€¼
        create_topic_rocketmq(args.topic, namesrv_addr, num_queues=args.num_consumers)

        baseline_metrics = start_remote_monitoring(remote_ips)

        from multiprocessing import Manager
        manager = Manager()
        producer_metrics = manager.list()
        consumer_metrics = manager.list()
        processes = []

        # (A) åœ¨æ­¤å¤„åˆ›å»º"æ€»é‡"ç›¸å…³çš„å…±äº«å˜é‡
        global_count = manager.Value('i', 0)      # æ•´å‹
        count_lock  = manager.Lock()             # å¹¶å‘é”
        global_stop = manager.Value('b', False)  # å¸ƒå°”

        # å¯åŠ¨æ¶ˆè´¹è€…è¿›ç¨‹
        for i in range(args.num_consumers):
            p = Process(target=consume_messages_rocketmq, args=(
                {
                    "namesrv_addr": namesrv_addr,
                    "consumer_group": "latency-test-group",
                    "global_count": global_count,
                    "count_lock": count_lock,
                    "global_stop": global_stop,
                    "total_messages": total_messages
                },
                args.topic,
                args.log_interval,
                consumer_metrics,
                i
            ))
            p.start()
            processes.append(p)

        # å¯åŠ¨ç”Ÿäº§è€…è¿›ç¨‹
        for i in range(args.num_producers):
            p = Process(target=produce_messages_rocketmq, args=(
                {
                    "namesrv_addr": namesrv_addr,
                    "producer_group": "throughput-test-group"
                },
                args.topic,
                args.messages_per_producer,
                args.log_interval,
                producer_metrics,
                i,
                args.message_size
            ))
            p.start()
            processes.append(p)

        for p in processes:
            p.join()

        remote_results = stop_remote_monitoring(remote_ips)

    else:
        print("ç›®å‰ä»…æ”¯æŒ --mq_type=kafka æˆ– --mq_type=rocketmq")
        exit(1)

    # ============ ç»Ÿä¸€çš„èµ„æºå¢é‡è®¡ç®— & ç»“æœæ±‡æ€»éƒ¨åˆ† ============
    processed_remote = []
    if remote_results:
        for item in remote_results:
            ip = item["ip"]
            baseline = baseline_metrics.get(ip, {})
            delta_cpu = item.get("avg_cpu", 0) - baseline.get("avg_cpu", 0)
            delta_mem = item.get("avg_mem", 0) - baseline.get("avg_mem", 0)
            processed_item = {
                "ip": ip,
                "avg_cpu": round(delta_cpu / 100.0, 4),
                "avg_mem_mb": round(delta_mem / (1024*1024), 2),
                "samples_count": item.get("samples_count", 0)
            }
            processed_remote.append(processed_item)

        avg_cpu_all = sum(
            item.get("avg_cpu", 0) - baseline_metrics.get(item["ip"], {}).get("avg_cpu", 0)
            for item in remote_results
        ) / len(remote_results) / 100.0

        avg_mem_all = sum(
            item.get("avg_mem", 0) - baseline_metrics.get(item["ip"], {}).get("avg_mem", 0)
            for item in remote_results
        ) / len(remote_results) / (1024*1024)

        print("\n===== æœåŠ¡å™¨èµ„æºä½¿ç”¨æƒ…å†µ =====")
        print(f"ä¸‰ä¸ªæœåŠ¡å™¨å¹³å‡ CPU å¢é‡: {avg_cpu_all:.4f}")
        print(f"ä¸‰ä¸ªæœåŠ¡å™¨å¹³å‡ å†…å­˜å¢é‡: {avg_mem_all:.2f} MB")
        for item in processed_remote:
            print(f"æœåŠ¡å™¨ {item['ip']}ï¼š CPU å¢é‡: {item['avg_cpu']:.4f}, "
                  f"å†…å­˜å¢é‡: {item['avg_mem_mb']:.2f} MB (é‡‡æ · {item['samples_count']} æ¬¡)")
    else:
        print("æœªè·å–åˆ°è¿œç¨‹èµ„æºæ•°æ®ã€‚")

    # ---- ç»Ÿè®¡ç”Ÿäº§è€…ã€æ¶ˆè´¹è€…metrics ----
    total_messages_produced = sum(item["messages"] for item in producer_metrics)
    max_producer_duration = max(item["duration"] for item in producer_metrics) if producer_metrics else 1
    overall_throughput_producers = total_messages_produced / max_producer_duration if max_producer_duration > 0 else 0

    avg_consumer_throughput = (sum(item["throughput"] for item in consumer_metrics) / len(consumer_metrics)
                               if consumer_metrics else 0)
    avg_latency = (sum(item["avg_latency"] for item in consumer_metrics) / len(consumer_metrics)
                   if consumer_metrics else 0)
    avg_p99_latency = (sum(item["p99_latency"] for item in consumer_metrics) / len(consumer_metrics)
                       if consumer_metrics else 0)
    avg_cold_start = (sum(item["cold_start_latency"] for item in consumer_metrics) / len(consumer_metrics)
                      if consumer_metrics else 0)

    print("\n===== ç»¼åˆæµ‹è¯•ç»“æœ =====")
    print(f"ç”Ÿäº§è€…: æ€»å‘é€æ¶ˆæ¯æ•°: {total_messages_produced}, å¹³å‡ååé‡: {overall_throughput_producers:.2f} msg/s")
    print("æ¶ˆè´¹è€…:")
    print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.6f} s")
    print(f"  è¿‘ä¼¼99%å»¶è¿Ÿ: {avg_p99_latency:.6f} s")
    print(f"  å¹³å‡å†·å¯åŠ¨å»¶è¿Ÿ: {avg_cold_start:.6f} s")
    print(f"  å¹³å‡ååé‡: {avg_consumer_throughput:.2f} msg/s")

    results_summary = {
        "parameters": {
            "mq_type": args.mq_type,
            "broker_address": args.broker_address,
            "topic": args.topic,
            "num_producers": args.num_producers,
            "num_consumers": args.num_consumers,
            "messages_per_producer": args.messages_per_producer,
            "log_interval": args.log_interval,
            "message_size": args.message_size,
            "remote_ips": remote_ips
        },
        "summary": {
            "total_messages_produced": total_messages_produced,
            "producer_avg_throughput": overall_throughput_producers,
            "consumer_avg_throughput": avg_consumer_throughput,
            "consumer_avg_latency": avg_latency,
            "consumer_p99_latency": avg_p99_latency,
            "consumer_cold_start_latency": avg_cold_start,
            "remote_servers": processed_remote
        }
    }

    results_dir = os.path.join("results", args.mq_type)
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    filename = os.path.join(
        results_dir,
        f"{args.num_consumers}consumer{args.num_producers}producer{args.messages_per_producer//1000}kmessages_{timestamp}.json"
    )
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results_summary, f, indent=4, ensure_ascii=False)
    print(f"\nç»“æœå·²ä¿å­˜è‡³ {filename}")
